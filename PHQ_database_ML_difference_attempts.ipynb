{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "data = pd.read_csv('edited_data.csv')\n",
    "X = data['Objectives']+'. '+ data['Description']\n",
    "X = X.str.replace('<ul>', '').str.replace('<li>','').str.replace('</li>','').\\\n",
    "str.replace('</ul>','').str.replace('1.','').str.replace('2.','').str.replace('3.','').str.replace('4.','').str.replace('5.','').\\\n",
    "str.replace('(','').str.replace(')','').str.replace('\\n','')\n",
    "X = X.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/penghanqiu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words('english')) \n",
    "not_delete = [\"but\", \"shan't\", \"wasn't\", \"couldn't\", \"didn't\", \"hadn't\", \"against\", \"no\", \"haven't\", \"shouldn't\", \"needn't\", \"wouldn't\", \"aren't\", \"mightn't\", \"won't\", \"isn't\", \"hasn't\", \"don't\", \"mustn't\", \"doesn't\", \"not\"]\n",
    "stop_words = [w for w in stop_words if w not in not_delete]\n",
    "#print(stop_words)\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        if word not in stop_words:\n",
    "            stem_sentence.append(porter.stem(word))\n",
    "            stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,X.count()):\n",
    "    #print(type(i))\n",
    "    X[i] = stemSentence(X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nOneVsRest_classifier.fit(X_train, y_train) \\npredicted = OneVsRest_classifier.predict(X_test)\\nprint(OneVsRest_classifier.score(X_test, y_test))\\nprint('OneVsRestClassifier_test:'+ str(np.mean(predicted == y_test)))\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#multilabel classifier: project tag %95\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "y2 = data['Project Tag']\n",
    "mlb = MultiLabelBinarizer()\n",
    "y2 = y2.fillna(\"N/A\")\n",
    "y2 = y2.str.split(', ')\n",
    "\n",
    "for i in range(0, len(y2)):\n",
    "    for j in range(0, len(y2[i])):\n",
    "        y2[i][j] = y2[i][j].replace(\"Community Event \", \"Community Event\")\\\n",
    "        .replace(\"Conference/Panel Discussion \", \"Conference/Panel Discussion\")\\\n",
    "        .replace(\"Educational Material \", \"Educational Material\")\\\n",
    "        .replace(\"Social Media \", \"Social Media\").replace(\"Survey \", \"Survey\")\\\n",
    "        .replace(\"Teaching Activity \", \"Teaching Activity\")\n",
    "    \n",
    "mlb_y2 = mlb.fit_transform(y2)\n",
    "\n",
    "#print(mlb.classes_)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, mlb_y2, test_size=0.3, random_state=52)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "#from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "# initialize classifier chains multi-label classifier\n",
    "\n",
    "\n",
    "Classifiers = [\n",
    "    #KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    ExtraTreeClassifier(),\n",
    "    #RadiusNeighborsClassifier(),\n",
    "    ExtraTreesClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    #MLPClassifier(),\n",
    "    #RidgeClassifierCV()\n",
    "    OneVsRestClassifier(SGDClassifier())\n",
    "    ]\n",
    "\n",
    "'''\n",
    "for classifier in Classifiers:\n",
    "    pipe = Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\\\n",
    "                    ('classifier', classifier)])\n",
    "    pipe.fit(X_train, y_train)   \n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % pipe.score(X_test, y_test))\n",
    "'''\n",
    "\n",
    "'''\n",
    "OneVsRest_classifier = Pipeline([\n",
    "('vectorizer', CountVectorizer(ngram_range=(1, 2),stop_words='english',strip_accents = 'unicode')),\n",
    "('tfidf', TfidfTransformer(sublinear_tf=True)),\n",
    "('clf', OneVsRestClassifier(SVC(C=1.0, kernel='linear')))])\n",
    "'''\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "Tfidf_vect = TfidfVectorizer()\n",
    "X_train = Tfidf_vect.fit_transform(X_train)\n",
    "X_test = Tfidf_vect.transform(X_test)\n",
    "\n",
    "# Fit an ensemble of SVM classifier chains and take the average prediction of all the chains.\n",
    "from sklearn.metrics import jaccard_score\n",
    "chains = [ClassifierChain(SVC(C=1.0, kernel='linear'), order='random', random_state=i)\n",
    "          for i in range(10)]\n",
    "for chain in chains:\n",
    "    chain.fit(X_train, y_train)\n",
    "    \n",
    "Y_pred_chains = np.array([chain.predict(X_test) for chain in\n",
    "                          chains])\n",
    "#chain_jaccard_scores = [jaccard_score(y_test, Y_pred_chain >= .5,\n",
    "#                                      average='samples')\n",
    "#                        for Y_pred_chain in Y_pred_chains]\n",
    "#Y_pred_ensemble = Y_pred_chains.mode()\n",
    "\n",
    "#ensemble_jaccard_score = jaccard_score(y_test,\n",
    "#                                       Y_pred_ensemble >= .5,\n",
    "#                                       average='samples')\n",
    "                                       \n",
    "#print('Chain_Classifier_test:'+ str(np.mean(Y_pred_ensemble == y_test)))\n",
    "#print(ensemble_jaccard_score)\n",
    "\n",
    "\n",
    "'''\n",
    "Chain_classifier = Pipeline([\n",
    "('vectorizer', CountVectorizer(ngram_range=(1, 2),stop_words='english',strip_accents = 'unicode')),\n",
    "('tfidf', TfidfTransformer(sublinear_tf=True)),\n",
    "('clf', ClassifierChain(SVC(C=1.0, kernel='linear')))])\n",
    "Chain_classifier.fit(X_train, y_train) \n",
    "predicted = Chain_classifier.predict(X_test)\n",
    "print(Chain_classifier.score(X_test, y_test))\n",
    "print('ChainClassifier_test:'+ str(np.mean(predicted == y_test)))\n",
    "'''\n",
    "\n",
    "'''\n",
    "OneVsRest_classifier.fit(X_train, y_train) \n",
    "predicted = OneVsRest_classifier.predict(X_test)\n",
    "print(OneVsRest_classifier.score(X_test, y_test))\n",
    "print('OneVsRestClassifier_test:'+ str(np.mean(predicted == y_test)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 1. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 1. 0.]\n",
      "  [0. 0. 0. ... 0. 1. 0.]\n",
      "  [0. 0. 1. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "a=Y_pred_chains\n",
    "m = stats.mode(a)\n",
    "print(m[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain_Classifier_test:0.9514660353305409\n"
     ]
    }
   ],
   "source": [
    "print('Chain_Classifier_test:'+ str(np.mean(m[0] == y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 2), preprocessor=None,\n",
       "                                 stop_words='english', strip_accents='unicode',\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=Non...\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=True,\n",
       "                                  use_idf=True)),\n",
       "                ('clf',\n",
       "                 OneVsRestClassifier(estimator=SVC(C=1.0, cache_size=200,\n",
       "                                                   class_weight=None, coef0=0.0,\n",
       "                                                   decision_function_shape='ovr',\n",
       "                                                   degree=3,\n",
       "                                                   gamma='auto_deprecated',\n",
       "                                                   kernel='linear', max_iter=-1,\n",
       "                                                   probability=False,\n",
       "                                                   random_state=None,\n",
       "                                                   shrinking=True, tol=0.001,\n",
       "                                                   verbose=False),\n",
       "                                     n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OneVsRest_classifier.fit(X, mlb_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv('initial_data.csv')\n",
    "X_new = new_data['Descriptions']\n",
    "X_new = X_new.str.replace('\\n', '').str.replace('\\t','')\n",
    "X_new = X_new.fillna('')\n",
    "predicted1 = OneVsRest_classifier.predict(X_new)\n",
    "output = mlb.inverse_transform(predicted1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将predict得到的结果放到csv里面\n",
    "import csv\n",
    "df = pd.DataFrame(output)\n",
    "df.to_csv(\"new_data.csv\", sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data scienc interdisciplinari field use scientif method , process \n"
     ]
    }
   ],
   "source": [
    "sentence = 'Data science is an interdisciplinary field that uses scientific methods, processes'\n",
    "a = stemSentence(sentence)\n",
    "print(a)\n",
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3大主要classifier：Outreach Category 85% 2选1\n",
    "\n",
    "y = data['Outreach Category']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "'''\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB()),])\n",
    "text_clf.fit(X_train, y_train) \n",
    "predicted = text_clf.predict(X_test)\n",
    "print('MultinomialNB_test:'+ str(np.mean(predicted == y_test)))\n",
    "'''\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf_2 = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),('tfidf', TfidfTransformer()),('clf', SGDClassifier(alpha=0.00015)),])\n",
    "text_clf_2.fit(X_train, y_train)\n",
    "predicted = text_clf_2.predict(X_test)\n",
    "print('SGDClassifier_test:'+ str(np.mean(predicted == y_test))) \n",
    "categories = ['Education and Public Engagement', 'Integrated Human Practices']\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, predicted, target_names = categories))\n",
    "\n",
    "'''\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "text_clf_3 = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),\\\n",
    "            ('clf', MLPClassifier(solver='lbfgs', alpha=1e-5,\\\n",
    "            hidden_layer_sizes=(50,100,100,100,100,50,50,50), random_state=1, \\\n",
    "            activation = 'relu', learning_rate = 'adaptive'))])\n",
    "text_clf_3.fit(X_train, y_train)\n",
    "predicted = text_clf_3.predict(X_test)\n",
    "print('Neural Network:'+ str(np.mean(predicted == y_test))) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multilabel classifier: project tag %95\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "y2 = data['Project Tag']\n",
    "mlb = MultiLabelBinarizer()\n",
    "y2 = y2.fillna(\"N/A\")\n",
    "y2 = y2.str.split(', ')\n",
    "\n",
    "for i in range(0, len(y2)):\n",
    "    for j in range(0, len(y2[i])):\n",
    "        y2[i][j] = y2[i][j].replace(\"Community Event \", \"Community Event\")\\\n",
    "        .replace(\"Conference/Panel Discussion \", \"Conference/Panel Discussion\")\\\n",
    "        .replace(\"Educational Material \", \"Educational Material\")\\\n",
    "        .replace(\"Social Media \", \"Social Media\").replace(\"Survey \", \"Survey\")\\\n",
    "        .replace(\"Teaching Activity \", \"Teaching Activity\")\n",
    "    \n",
    "mlb_y2 = mlb.fit_transform(y2)\n",
    "\n",
    "print(mlb.classes_)\n",
    "print(mlb_y2[100:150])\n",
    "print(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, mlb_y2, test_size=0.3, random_state=52)\n",
    "#print(y_train[100:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "[('Survey',), (), ('Consult Experts',), ('Teaching Activity',), ('Consult Experts',), (), ('Consult Experts',), ('Project Presentation',), (), ('Consult Experts',), (), (), ('Consult Experts',), ('Consult Experts',), ('Consult Experts',), ('Consult Experts', 'Survey'), ('Educational Material',), (), (), (), ('Consult Experts',), ('Consult Experts',), ('Consult Experts',), ('Consult Experts',), ('Consult Experts',), (), ('Educational Material',), ('Consult Experts',)]\n"
     ]
    }
   ],
   "source": [
    "print(str(predicted[2]))\n",
    "#encode后y的样子\n",
    "#print(mlb_y2)\n",
    "#将标签变回string\n",
    "print(mlb.inverse_transform(predicted[2:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将predict得到的结果放到csv里面\n",
    "\n",
    "import csv\n",
    "results = mlb.inverse_transform(predicted)\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"new_data.csv\", sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_zeros_test:0.923511200145693\n",
      "(646, 17)\n"
     ]
    }
   ],
   "source": [
    "#测试将预测值全部变成0\n",
    "p_0 = np.zeros((646,17))\n",
    "print('all_zeros_test:'+ str(np.mean(p_0 == y_test)))\n",
    "print(predicted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = ['<ul><li> Organize a \"Biotech Day\" and present the project to a large group of students</li><li> Address safety issues in ethics of synthetic biology</li><li> Answer their questions about synbio and the project </ul> Organized a Biotech Day for two schools to teach about iGEM, synthetic biology, and safety.', \n",
    "            'Teach high school students about synthetic biology by presenting their iGEM project during the student summer STEM outreach camp. Introduce the students to synthetic biology and answer questions and discuss with students afterwards who are interested in synthetic biology.']\n",
    "predicted1 = multilabel_classifier.predict(docs_new)\n",
    "for doc, category in zip(docs_new, predicted1):\n",
    "    print('%r => %s' % (doc, category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multilabel classifier: Audience %95\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "y2 = data['Audience']\n",
    "mlb = MultiLabelBinarizer()\n",
    "y2 = y2.fillna(\"N/A\")\n",
    "y2 = y2.str.split(', ')\n",
    "\n",
    "\n",
    "a = mlb.fit_transform(y2)\n",
    "\n",
    "#print(mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试其他classifier: Outreach Category 2选1 #NuSVC最好 82%\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    NuSVC(probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier()\n",
    "    ]\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\\\n",
    "                    ('classifier', classifier)])\n",
    "    pipe.fit(X_train, y_train)   \n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#算的training accuracy，之前的都是test accuracy。\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB()),])\n",
    "text_clf.fit(X_train, y_train) \n",
    "predicted = text_clf.predict(X_train)\n",
    "print('MultinomialNB_train:'+ str(np.mean(predicted == y_train)))\n",
    "\n",
    "text_clf_2 = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),('tfidf', TfidfTransformer()),('clf', SGDClassifier(alpha=0.0002)),])\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "#text_clf_2 = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),('tfidf', TfidfTransformer()),('clf', MLPClassifier()),])\n",
    "text_clf_2.fit(X_train, y_train)\n",
    "predicted = text_clf_2.predict(X_train)\n",
    "print('SGDClassifier_train:'+ str(np.mean(predicted == y_train))) \n",
    "categories = ['Education and Public Engagement', 'Integrated Human Practices']\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_train, predicted, target_names = categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single sample prediction\n",
    "docs_new = ['Visit Snofarm, a small dairy farm with 90 cows about an hour away from campus. Learn about the milking process from the owner of the farm. Learn about how bovine mastitis affects people in the farming industry.', \n",
    "            'Teach high school students about synthetic biology by presenting their iGEM project during the student summer STEM outreach camp. Introduce the students to synthetic biology and answer questions and discuss with students afterwards who are interested in synthetic biology.']\n",
    "predicted1 = text_clf_2.predict(docs_new)\n",
    "for doc, category in zip(docs_new, predicted1):\n",
    "    print('%r => %s' % (doc, category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search for best hyperparameters/parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1),(1,2),(1,3)],'tfidf__use_idf': (True, False),'clf__alpha': (0.00095, 0.0009,0.0097)}\n",
    "#parameters={'clf__alpha': (0.11,0.1,0.09)}\n",
    "gs_clf = GridSearchCV(text_clf_2, parameters, cv=5, iid=False, n_jobs=-1)\n",
    "#speed up\n",
    "gs_clf = gs_clf.fit(X_train, y_train)\n",
    "print(gs_clf.best_score_)\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n",
    "#结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#载入别人train好的word embedding，估计run 15min\n",
    "with open(\"glove.6B.50d.txt\", \"rb\") as lines:\n",
    "    w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
    "           for line in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(word2vec.values())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(strip_accents = 'unicode', analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])\n",
    "#GloVe:太慢了1个小时都没run出来\n",
    "OneVsRest_classifier = Pipeline([\n",
    "(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)),\n",
    "('clf', OneVsRestClassifier(SVC(C=1.0, kernel='linear')))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

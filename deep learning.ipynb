{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "data = pd.read_csv('edited_data.csv')\n",
    "X = data['Objectives']+'. '+ data['Description']\n",
    "X = X.str.replace('<ul>', '').str.replace('<li>','').str.replace('</li>','').\\\n",
    "str.replace('</ul>','').str.replace('1.','').str.replace('2.','').str.replace('3.','').str.replace('4.','').str.replace('5.','').\\\n",
    "str.replace('(','').str.replace(')','').str.replace('\\n','')\n",
    "X = X.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4312    3 3020 1764 3021   41  501  789   29 4313    8  533   69  747\n",
      " 2020    8  501   69  747    7  428   11    8 3020 1764 3021    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000, lower=True)\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "x = pad_sequences(sequences, padding='post', maxlen=80)\n",
    "print(x[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "y2 = data['Project Tag']\n",
    "mlb = MultiLabelBinarizer()\n",
    "y2 = y2.fillna(\"N/A\")\n",
    "y2 = y2.str.split(', ')\n",
    "\n",
    "for i in range(0, len(y2)):\n",
    "    for j in range(0, len(y2[i])):\n",
    "        y2[i][j] = y2[i][j].replace(\"Community Event \", \"Community Event\")\\\n",
    "        .replace(\"Conference/Panel Discussion \", \"Conference/Panel Discussion\")\\\n",
    "        .replace(\"Educational Material \", \"Educational Material\")\\\n",
    "        .replace(\"Social Media \", \"Social Media\").replace(\"Survey \", \"Survey\")\\\n",
    "        .replace(\"Teaching Activity \", \"Teaching Activity\")\n",
    "    \n",
    "mlb_y2 = mlb.fit_transform(y2)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, mlb_y2, test_size=0.3, random_state=52)\n",
    "\n",
    "#print(len(mlb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 90, 50)            379650    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 90, 50)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 88, 300)           45300     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_10 (Glo (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 17)                5117      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 17)                0         \n",
      "=================================================================\n",
      "Total params: 430,067\n",
      "Trainable params: 430,067\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1356 samples, validate on 151 samples\n",
      "Epoch 1/20\n",
      "1356/1356 [==============================] - 2s 1ms/step - loss: 0.6663 - categorical_accuracy: 0.0841 - val_loss: 0.6400 - val_categorical_accuracy: 0.1192\n",
      "Epoch 2/20\n",
      "1356/1356 [==============================] - 1s 734us/step - loss: 0.6155 - categorical_accuracy: 0.1431 - val_loss: 0.5874 - val_categorical_accuracy: 0.1987\n",
      "Epoch 3/20\n",
      "1356/1356 [==============================] - 1s 692us/step - loss: 0.5534 - categorical_accuracy: 0.2559 - val_loss: 0.5211 - val_categorical_accuracy: 0.2252\n",
      "Epoch 4/20\n",
      "1356/1356 [==============================] - 1s 635us/step - loss: 0.4781 - categorical_accuracy: 0.2773 - val_loss: 0.4421 - val_categorical_accuracy: 0.2583\n",
      "Epoch 5/20\n",
      "1356/1356 [==============================] - 1s 653us/step - loss: 0.3944 - categorical_accuracy: 0.2928 - val_loss: 0.3602 - val_categorical_accuracy: 0.2517\n",
      "Epoch 6/20\n",
      "1356/1356 [==============================] - 1s 640us/step - loss: 0.3176 - categorical_accuracy: 0.2817 - val_loss: 0.2945 - val_categorical_accuracy: 0.2318\n",
      "Epoch 7/20\n",
      "1356/1356 [==============================] - 1s 810us/step - loss: 0.2670 - categorical_accuracy: 0.2684 - val_loss: 0.2588 - val_categorical_accuracy: 0.2318\n",
      "Epoch 8/20\n",
      "1356/1356 [==============================] - 1s 671us/step - loss: 0.2459 - categorical_accuracy: 0.2677 - val_loss: 0.2461 - val_categorical_accuracy: 0.2318\n",
      "Epoch 9/20\n",
      "1356/1356 [==============================] - 1s 613us/step - loss: 0.2401 - categorical_accuracy: 0.2677 - val_loss: 0.2412 - val_categorical_accuracy: 0.2318\n",
      "Epoch 10/20\n",
      "1356/1356 [==============================] - 1s 614us/step - loss: 0.2354 - categorical_accuracy: 0.2677 - val_loss: 0.2375 - val_categorical_accuracy: 0.2318\n",
      "Epoch 11/20\n",
      "1356/1356 [==============================] - 1s 620us/step - loss: 0.2309 - categorical_accuracy: 0.2684 - val_loss: 0.2347 - val_categorical_accuracy: 0.2384\n",
      "Epoch 12/20\n",
      "1356/1356 [==============================] - 1s 616us/step - loss: 0.2273 - categorical_accuracy: 0.2869 - val_loss: 0.2330 - val_categorical_accuracy: 0.2781\n",
      "Epoch 13/20\n",
      "1356/1356 [==============================] - 1s 626us/step - loss: 0.2247 - categorical_accuracy: 0.3201 - val_loss: 0.2315 - val_categorical_accuracy: 0.2848\n",
      "Epoch 14/20\n",
      "1356/1356 [==============================] - 1s 648us/step - loss: 0.2228 - categorical_accuracy: 0.3149 - val_loss: 0.2296 - val_categorical_accuracy: 0.2848\n",
      "Epoch 15/20\n",
      "1356/1356 [==============================] - 1s 634us/step - loss: 0.2208 - categorical_accuracy: 0.3112 - val_loss: 0.2274 - val_categorical_accuracy: 0.2781\n",
      "Epoch 16/20\n",
      "1356/1356 [==============================] - 1s 782us/step - loss: 0.2184 - categorical_accuracy: 0.3164 - val_loss: 0.2250 - val_categorical_accuracy: 0.2848\n",
      "Epoch 17/20\n",
      "1356/1356 [==============================] - 1s 698us/step - loss: 0.2158 - categorical_accuracy: 0.3215 - val_loss: 0.2225 - val_categorical_accuracy: 0.2848\n",
      "Epoch 18/20\n",
      "1356/1356 [==============================] - 1s 634us/step - loss: 0.2131 - categorical_accuracy: 0.3171 - val_loss: 0.2198 - val_categorical_accuracy: 0.2781\n",
      "Epoch 19/20\n",
      "1356/1356 [==============================] - 1s 638us/step - loss: 0.2099 - categorical_accuracy: 0.3149 - val_loss: 0.2169 - val_categorical_accuracy: 0.2848\n",
      "Epoch 20/20\n",
      "1356/1356 [==============================] - 1s 626us/step - loss: 0.2065 - categorical_accuracy: 0.3363 - val_loss: 0.2139 - val_categorical_accuracy: 0.3179\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten, GlobalMaxPool1D, Dropout, Conv1D, LSTM\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "filter_length = 300\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, \n",
    "                           output_dim= 50, input_length=90))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv1D(filter_length, 3, padding='valid', activation='relu', strides=1))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(len(mlb.classes_)))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(),\n",
    "    EarlyStopping(patience=4),\n",
    "    ModelCheckpoint(filepath='model-conv1d.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    #class_weight=class_weight,\n",
    "                    epochs=20,\n",
    "                    batch_size=300,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646/646 [==============================] - 1s 810us/step\n",
      "loss: 0.2147432404221396\n",
      "categorical_accuracy: 0.35294117651672186\n",
      "Training Accuracy: 0.3550\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "cnn_model = keras.models.load_model('model-conv1d.h5')\n",
    "metrics = cnn_model.evaluate(X_test, y_test)\n",
    "print(\"{}: {}\".format(model.metrics_names[0], metrics[0]))\n",
    "print(\"{}: {}\".format(model.metrics_names[1], metrics[1]))\n",
    "\n",
    "loss, accuracy = cnn_model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
